"""
Training module for Eye Disease Classification - FIXED VERSION
"""

import os
import json
import time
import logging
import mindspore as ms
import mindspore.nn as nn
import mindspore.ops as ops
from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor, Callback
from mindspore.train import Model
from mindspore import context
from mindspore import save_checkpoint, load_checkpoint, load_param_into_net
from mindspore.common import set_seed
from mindspore.ops import operations as P
from mindspore.ops import composite as C
from mindspore.ops import functional as F
from mindspore.nn.metrics import Accuracy
from mindspore.nn.learning_rate_schedule import CosineDecayLR
from datetime import datetime
from mindspore.dataset import ImageFolderDataset
from mindspore.dataset.transforms import c_transforms as C
from mindspore.dataset.vision import RandomResizedCrop, RandomHorizontalFlip, RandomColorAdjust, Resize, CenterCrop, Normalize, HWC2CHW, Decode, RandomVerticalFlip, RandomRotation
import numpy as np

from src.config.config import Config
from src.models.resnet import get_model

class EarlyStopping(Callback):
    """æ—©åœå›è°ƒ"""
    def __init__(self, patience=10, min_delta=0.001):
        super(EarlyStopping, self).__init__()
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = float('inf')
        self.wait = 0
        self.stopped_epoch = 0
        self.best_epoch = 0
        self.best_weights = None

    def on_epoch_end(self, run_context):
        """æ¯ä¸ªepochç»“æŸæ—¶è°ƒç”¨"""
        cb_params = run_context.original_args()
        current_loss = cb_params.get("net_outputs", float('inf'))
        
        if current_loss < self.best_loss - self.min_delta:
            self.best_loss = current_loss
            self.wait = 0
            self.best_epoch = cb_params.cur_epoch_num
            self.best_weights = cb_params.train_network.parameters_dict()
        else:
            self.wait += 1
            if self.wait >= self.patience:
                self.stopped_epoch = cb_params.cur_epoch_num
                run_context.request_stop()
                print(f"\nEarly stopping triggered at epoch {self.stopped_epoch}")
                print(f"Best loss: {self.best_loss:.4f} at epoch {self.best_epoch}")

class ValidationMonitor(Callback):
    """éªŒè¯ç›‘æ§å™¨"""
    def __init__(self, network, eval_dataset, patience=10, min_delta=0.001):
        super(ValidationMonitor, self).__init__()
        self.network = network
        self.eval_dataset = eval_dataset
        self.patience = patience
        self.min_delta = min_delta
        self.best_accuracy = 0.0
        self.wait = 0
        self.stopped_epoch = 0
        self.best_epoch = 0

    def on_train_epoch_end(self, run_context):
        """æ¯ä¸ªè®­ç»ƒepochç»“æŸæ—¶çš„å›è°ƒ"""
        cb_params = run_context.original_args()
        epoch = cb_params.cur_epoch_num
        
        # è®¾ç½®ç½‘ç»œä¸ºè¯„ä¼°æ¨¡å¼
        self.network.set_train(False)
        
        # è®¡ç®—éªŒè¯é›†å‡†ç¡®ç‡
        correct = 0
        total = 0
        for data in self.eval_dataset.create_dict_iterator():
            images = data['image']
            labels = data['label']
            outputs = self.network(images)
            pred = outputs.argmax(1)
            correct += (pred == labels).sum().asnumpy()
            total += labels.shape[0]
        
        accuracy = correct / total
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ—©åœ
        if accuracy > self.best_accuracy + self.min_delta:
            self.best_accuracy = accuracy
            self.wait = 0
            self.best_epoch = epoch
        else:
            self.wait += 1
            if self.wait >= self.patience:
                self.stopped_epoch = epoch
                run_context.request_stop()
                print(f"\nEarly stopping triggered at epoch {epoch}")
                print(f"Best accuracy: {self.best_accuracy:.2%} at epoch {self.best_epoch}")
        
        print(f"\nValidation accuracy: {accuracy:.2%}")
        
        # æ¢å¤è®­ç»ƒæ¨¡å¼
        self.network.set_train(True)

class TrainingLogger(Callback):
    """è®­ç»ƒæ—¥å¿—è®°å½•å™¨"""
    def __init__(self, log_dir):
        self.log_dir = log_dir
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file = os.path.join(log_dir, f"training_log_{self.timestamp}.json")
        self.metrics_file = os.path.join(log_dir, f"metrics_{self.timestamp}.json")
        
        # åˆå§‹åŒ–æ—¥å¿—æ•°æ®
        self.log_data = {
            "timestamp": self.timestamp,
            "training_start": datetime.now().isoformat(),
            "epochs": [],
            "best_accuracy": 0.0,
            "best_epoch": 0,
            "training_end": None,
            "total_time": None,
            "final_metrics": None
        }
        
        # åˆå§‹åŒ–æŒ‡æ ‡æ•°æ®
        self.metrics_data = {
            "timestamp": self.timestamp,
            "epochs": []
        }
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs(log_dir, exist_ok=True)
        
        # ä¿å­˜åˆå§‹é…ç½®
        self.config_path = Config.save_config(log_dir)
        self.log_data["config_path"] = self.config_path

    def on_train_epoch_begin(self, run_context):
        """æ¯ä¸ªè®­ç»ƒepochå¼€å§‹æ—¶çš„å›è°ƒ"""
        cb_params = run_context.original_args()
        epoch = cb_params.cur_epoch_num
        self.epoch_start_time = time.time()

    def on_train_epoch_end(self, run_context):
        """æ¯ä¸ªè®­ç»ƒepochç»“æŸæ—¶çš„å›è°ƒ"""
        cb_params = run_context.original_args()
        epoch = cb_params.cur_epoch_num
        epoch_time = time.time() - self.epoch_start_time
        
        # è®°å½•epochä¿¡æ¯
        epoch_info = {
            "epoch": epoch,
            "time": epoch_time
        }
        
        # å°è¯•è·å–æŸå¤±å€¼
        if hasattr(cb_params, 'net_outputs'):
            loss = cb_params.net_outputs
            if isinstance(loss, (list, tuple)):
                loss = loss[0]
            epoch_info["loss"] = float(loss.asnumpy())
        
        self.log_data["epochs"].append(epoch_info)
        self.metrics_data["epochs"].append(epoch_info)
        
        # ä¿å­˜æ—¥å¿—
        self._save_logs()

    def on_train_end(self, run_context):
        """è®­ç»ƒç»“æŸæ—¶çš„å›è°ƒ"""
        cb_params = run_context.original_args()
        
        # å°è¯•è·å–æœ€ç»ˆæŒ‡æ ‡
        try:
            if hasattr(cb_params, 'metrics'):
                self.log_data["final_metrics"] = cb_params.metrics
        except Exception as e:
            print(f"Warning: Could not get final metrics: {str(e)}")
        
        self.log_data["training_end"] = datetime.now().isoformat()
        self.log_data["total_time"] = time.time() - self.epoch_start_time
        self._save_logs()

    def _save_logs(self):
        """ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶"""
        with open(self.log_file, 'w', encoding='utf-8') as f:
            json.dump(self.log_data, f, indent=4, ensure_ascii=False)
        
        with open(self.metrics_file, 'w', encoding='utf-8') as f:
            json.dump(self.metrics_data, f, indent=4, ensure_ascii=False)

class FocalLoss(nn.Cell):
    """Focal Lossç”¨äºå¤„ç†æç«¯ç±»åˆ«ä¸å¹³è¡¡
    
    Focal Loss = -Î±(1-p_t)^Î³ * log(p_t)
    """
    def __init__(self, num_classes, alpha=None, gamma=2.0, class_weights=None):
        super(FocalLoss, self).__init__()
        self.num_classes = num_classes
        self.gamma = gamma
        self.class_weights = class_weights
        
        # è®¾ç½®alphaæƒé‡
        if alpha is None:
            self.alpha = ms.Tensor([1.0] * num_classes, dtype=ms.float32)
        else:
            self.alpha = ms.Tensor(alpha, dtype=ms.float32)
        
        # MindSporeæ“ä½œ
        self.softmax = ms.ops.Softmax(axis=1)
        self.log_softmax = ms.ops.LogSoftmax(axis=1)
        self.onehot = ms.ops.OneHot()
        self.gather = ms.ops.GatherD()
        self.pow = ms.ops.Pow()
        self.expand_dims = ms.ops.ExpandDims()
    
    def construct(self, logits, targets):
        # è®¡ç®—æ¦‚ç‡
        log_probs = self.log_softmax(logits)
        probs = self.softmax(logits)
        
        # åˆ›å»ºone-hotç¼–ç 
        targets_one_hot = self.onehot(targets, self.num_classes, 
                                     ms.Tensor(1.0, ms.float32), 
                                     ms.Tensor(0.0, ms.float32))
        
        # è·å–ç›®æ ‡ç±»åˆ«çš„æ¦‚ç‡
        p_t = (probs * targets_one_hot).sum(axis=1)
        
        # è·å–ç›®æ ‡ç±»åˆ«çš„logæ¦‚ç‡  
        log_p_t = (log_probs * targets_one_hot).sum(axis=1)
        
        # è·å–alphaæƒé‡
        alpha_t = self.gather(self.alpha, 0, targets.view(-1))
        
        # è®¡ç®—focalæƒé‡: (1-p_t)^gamma
        focal_weight = self.pow((1.0 - p_t), self.gamma)
        
        # è®¡ç®—focal loss
        focal_loss = -alpha_t * focal_weight * log_p_t
        
        # åº”ç”¨ç±»åˆ«æƒé‡
        if self.class_weights is not None:
            class_weight_t = self.gather(self.class_weights, 0, targets.view(-1))
            focal_loss = focal_loss * class_weight_t
        
        return focal_loss.mean()

class AdvancedLoss(nn.Cell):
    """ç»“åˆå¤šç§æŠ€æœ¯çš„é«˜çº§æŸå¤±å‡½æ•°"""
    def __init__(self, num_classes, class_weights, focal_gamma=2.0, label_smoothing=0.1):
        super(AdvancedLoss, self).__init__()
        self.num_classes = num_classes
        self.class_weights = class_weights
        self.label_smoothing = label_smoothing
        
        # Focal Loss
        self.focal_loss = FocalLoss(
            num_classes=num_classes,
            gamma=focal_gamma,
            class_weights=class_weights
        )
        
        # æ ‡å‡†äº¤å‰ç†µ
        self.ce_loss = nn.CrossEntropyLoss(reduction='none')
        
    def construct(self, logits, targets):
        # ä¸»è¦ä½¿ç”¨Focal Loss
        focal_loss = self.focal_loss(logits, targets)
        
        # æ·»åŠ å°‘é‡æ ‡å‡†äº¤å‰ç†µä½œä¸ºæ­£åˆ™åŒ–
        ce_loss = self.ce_loss(logits, targets)
        if self.class_weights is not None:
            weight_mask = ms.ops.GatherD()(self.class_weights, 0, targets)
            ce_loss = ce_loss * weight_mask
        ce_loss = ce_loss.mean()
        
        # Label smoothing
        if self.label_smoothing > 0:
            log_probs = ms.ops.LogSoftmax(axis=1)(logits)
            smooth_loss = -log_probs.mean(axis=1)
            if self.class_weights is not None:
                weight_mask = ms.ops.GatherD()(self.class_weights, 0, targets)
                smooth_loss = smooth_loss * weight_mask
            smooth_loss = smooth_loss.mean()
            
            # ç»„åˆæŸå¤±
            total_loss = 0.7 * focal_loss + 0.2 * ce_loss + 0.1 * smooth_loss
        else:
            total_loss = 0.8 * focal_loss + 0.2 * ce_loss
        
        return total_loss

def validate_image(image_path):
    """éªŒè¯å›¾åƒæ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"""
    try:
        from PIL import Image
        import numpy as np
        
        # å°è¯•æ‰“å¼€å›¾åƒ
        with Image.open(image_path) as img:
            # æ£€æŸ¥å›¾åƒæ¨¡å¼
            if img.mode not in ['RGB', 'RGBA', 'L']:
                return False
            
            # æ£€æŸ¥å›¾åƒå¤§å°
            if img.size[0] < 32 or img.size[1] < 32:
                return False
            
            # å°è¯•è½¬æ¢ä¸ºnumpyæ•°ç»„
            img_array = np.array(img)
            
            # æ£€æŸ¥æ•°ç»„å½¢çŠ¶
            if len(img_array.shape) < 2:
                return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„åƒç´ å€¼
            if img_array.size == 0:
                return False
            
            return True
            
    except Exception as e:
        print(f"Image validation error for {image_path}: {e}")
        return False

def clean_dataset(dataset_path):
    """æ¸…ç†æ•°æ®é›†ä¸­çš„æ— æ•ˆå›¾åƒ"""
    import os
    import shutil
    
    invalid_files = []
    backup_dir = os.path.join(os.path.dirname(dataset_path), "invalid_images")
    
    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                file_path = os.path.join(root, file)
                if not validate_image(file_path):
                    invalid_files.append(file_path)
    
    if invalid_files:
        os.makedirs(backup_dir, exist_ok=True)
        print(f"Found {len(invalid_files)} invalid image files:")
        for file_path in invalid_files:
            print(f"  - {file_path}")
            # ç§»åŠ¨æ— æ•ˆæ–‡ä»¶åˆ°å¤‡ä»½ç›®å½•
            rel_path = os.path.relpath(file_path, dataset_path)
            backup_path = os.path.join(backup_dir, rel_path)
            os.makedirs(os.path.dirname(backup_path), exist_ok=True)
            try:
                shutil.move(file_path, backup_path)
                print(f"    Moved to: {backup_path}")
            except Exception as e:
                print(f"    Failed to move: {e}")
    
    return len(invalid_files)

def get_fast_dataset(dataset_path, is_training=True):
    """è·å–å¿«é€Ÿè®­ç»ƒæ•°æ®é›† - å‡å°‘æ•°æ®å¢å¼ºä»¥æé«˜é€Ÿåº¦"""
    # é¦–å…ˆæ¸…ç†æ— æ•ˆå›¾åƒ
    print(f"Validating images in {dataset_path}...")
    invalid_count = clean_dataset(dataset_path)
    if invalid_count > 0:
        print(f"Removed {invalid_count} invalid images")
    
    # å¢å¼ºçš„æ•°æ®å¢å¼º - å¸®åŠ©ç¨€å°‘ç±»åˆ«å­¦ä¹ 
    if is_training:
        transform = [
            # æ›´å¼ºçš„æ•°æ®å˜æ¢ä»¥å¢åŠ æ ·æœ¬å¤šæ ·æ€§
            Resize((Config.IMAGE_SIZE + 32, Config.IMAGE_SIZE + 32)),
            RandomResizedCrop(Config.IMAGE_SIZE, scale=(0.8, 1.0), ratio=(0.8, 1.2)),
            RandomHorizontalFlip(prob=0.5),
            RandomVerticalFlip(prob=0.3),  # å¢åŠ å‚ç›´ç¿»è½¬
            RandomRotation(degrees=15),     # å¢åŠ æ—‹è½¬è§’åº¦
            # å¢å¼ºçš„é¢œè‰²å˜æ¢
            RandomColorAdjust(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),
            # å½’ä¸€åŒ–
            Normalize(Config.MEAN, Config.STD),
            HWC2CHW()
        ]
    else:
        transform = [
            # éªŒè¯æ—¶åªè¿›è¡Œå¿…è¦çš„å˜æ¢
            Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),
            Normalize(Config.MEAN, Config.STD),
            HWC2CHW()
        ]
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = ImageFolderDataset(
        dataset_path,
        num_parallel_workers=1,  # å‡å°‘å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°ä»¥ä¾¿è°ƒè¯•
        shuffle=is_training,
        decode=True  # ç¡®ä¿è‡ªåŠ¨è§£ç 
    )
    
    # åº”ç”¨è½¬æ¢
    dataset = dataset.map(
        operations=transform,
        input_columns="image",
        num_parallel_workers=1,  # å‡å°‘å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°ä»¥ä¾¿è°ƒè¯•
        python_multiprocessing=False  # ç¦ç”¨å¤šè¿›ç¨‹ä»¥é¿å…é—®é¢˜
    )
    
    # è®¾ç½®æ‰¹æ¬¡å¤§å°
    dataset = dataset.batch(Config.BATCH_SIZE, drop_remainder=is_training)
    
    return dataset

def get_dataset(dataset_path, is_training=True):
    """è·å–æ•°æ®é›†"""
    # é¦–å…ˆæ¸…ç†æ— æ•ˆå›¾åƒ
    print(f"Validating images in {dataset_path}...")
    invalid_count = clean_dataset(dataset_path)
    if invalid_count > 0:
        print(f"Removed {invalid_count} invalid images")
    
    # æ•°æ®å¢å¼º - é’ˆå¯¹ç¨€å°‘ç±»åˆ«çš„å¼ºåŒ–å¢å¼ºç­–ç•¥
    if is_training:
        transform = [
            # æ›´å¼ºçš„æ•°æ®å˜æ¢ä»¥å¢åŠ æ ·æœ¬å¤šæ ·æ€§
            Resize((Config.IMAGE_SIZE + 32, Config.IMAGE_SIZE + 32)),
            RandomResizedCrop(Config.IMAGE_SIZE, scale=(0.8, 1.0), ratio=(0.8, 1.2)),
            # å¢åŠ ç¿»è½¬æ¦‚ç‡
            RandomHorizontalFlip(prob=0.5),
            RandomVerticalFlip(prob=0.3),  # å¢åŠ å‚ç›´ç¿»è½¬
            # å¢åŠ æ—‹è½¬è§’åº¦å¸®åŠ©ç¨€å°‘ç±»åˆ«
            RandomRotation(degrees=15),
            # å¢å¼ºçš„é¢œè‰²å˜æ¢
            RandomColorAdjust(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
            # å½’ä¸€åŒ–
            Normalize(Config.MEAN, Config.STD),
            HWC2CHW()
        ]
    else:
        transform = [
            # éªŒè¯æ—¶åªè¿›è¡Œå¿…è¦çš„å˜æ¢
            Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),
            Normalize(Config.MEAN, Config.STD),
            HWC2CHW()
        ]
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = ImageFolderDataset(
        dataset_path,
        num_parallel_workers=1,  # å‡å°‘å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°ä»¥ä¾¿è°ƒè¯•
        shuffle=is_training,
        decode=True  # ç¡®ä¿è‡ªåŠ¨è§£ç 
    )
    
    # åº”ç”¨è½¬æ¢
    dataset = dataset.map(
        operations=transform,
        input_columns="image",
        num_parallel_workers=1,  # å‡å°‘å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°ä»¥ä¾¿è°ƒè¯•
        python_multiprocessing=False  # ç¦ç”¨å¤šè¿›ç¨‹ä»¥é¿å…é—®é¢˜
    )
    
    # è®¾ç½®æ‰¹æ¬¡å¤§å°
    dataset = dataset.batch(Config.BATCH_SIZE, drop_remainder=is_training)
    
    return dataset

def get_lr_scheduler(epochs, steps_per_epoch):
    """è·å–å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    # ä½¿ç”¨æ›´ç®€å•çš„ä½™å¼¦é€€ç«å­¦ä¹ ç‡
    decay_steps = epochs * steps_per_epoch
    
    lr_scheduler = CosineDecayLR(
        min_lr=Config.MIN_LR,
        max_lr=Config.MAX_LR,
        decay_steps=decay_steps
    )
    
    return lr_scheduler

def calculate_class_weights(dataset_path):
    """åŸºäºæ ·æœ¬æ•°é‡è®¡ç®—ç±»åˆ«æƒé‡ - å¼ºåŒ–å°ç±»åˆ«"""
    
    # ç”¨æˆ·è°ƒæ•´åçš„ç±»åˆ«åˆ†å¸ƒ
    actual_counts = {
        "g1-ageDegeneration": 213,
        "g1-cataract": 235,
        "g1-diabetes": 313,
        "g1-glaucoma": 228,
        "g1-myopia": 186,
        "g2-hypertension": 103,
        "g2-normal": 299,
        "g2-others": 301
    }
    
    total_samples = sum(actual_counts.values())
    
    # æ›´æ¿€è¿›çš„æƒé‡ç­–ç•¥ - å¹³æ–¹æ ¹å€’æ•°ï¼Œå¼ºåŒ–ç¨€å°‘ç±»åˆ«
    weights = {}
    
    for class_name, count in actual_counts.items():
        # ä½¿ç”¨å¹³æ–¹æ ¹å€’æ•°ï¼Œç»™ç¨€å°‘ç±»åˆ«æ›´å¤§æƒé‡
        weight = (total_samples / count) ** 0.7  # æŒ‡æ•°å°äº1ï¼Œå‡ç¼“æƒé‡å·®å¼‚
        weights[class_name] = weight
    
    # æ¸©å’Œçš„æƒé‡è°ƒæ•´ç­–ç•¥
    # é€‚åº¦æå‡å›°éš¾ç±»åˆ«
    problem_classes = ["g2-hypertension"]
    for class_name in problem_classes:
        weights[class_name] *= 2.0  # é€‚åº¦æå‡
    
    # æ¢å¤æ‰€æœ‰ç±»åˆ«çš„åŸºç¡€å­¦ä¹ èƒ½åŠ›
    # ä¸è¿‡åº¦å‹åˆ¶ä»»ä½•ç±»åˆ«
    
    # è½¬æ¢ä¸ºæ•°ç»„
    weight_list = [weights[class_name] for class_name in Config.CLASS_NAMES]
    weight_array = np.array(weight_list, dtype=np.float32)
    
    # æ ‡å‡†åŒ–æƒé‡ï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸
    weight_array = weight_array / weight_array.mean() * 2.0
    
    # æ—¥å¿—è¾“å‡º
    logging.info("=== å¼ºåŒ–æƒé‡ç­–ç•¥ ===")
    logging.info("ä¸ºç¨€å°‘ç±»åˆ«æä¾›æ›´å¼ºçš„æƒé‡æ”¯æŒ")
    
    max_ratio = weight_array.max() / weight_array.min()
    logging.info(f"æƒé‡æ¯”ä¾‹èŒƒå›´: 1:{max_ratio:.1f}")
    
    for i, (class_name, weight) in enumerate(zip(Config.CLASS_NAMES, weight_array)):
        count = actual_counts[class_name]
        percentage = count / total_samples * 100
        logging.info(f"{class_name}: æ ·æœ¬æ•°={count} ({percentage:.1f}%), æƒé‡={weight:.2f}")
    
    return weight_array

def get_balanced_training_strategy(train_dataset, class_weights):
    """è·å–å¹³è¡¡è®­ç»ƒç­–ç•¥ - ç®€åŒ–ç‰ˆæœ¬"""
    
    logging.info("=== è®­ç»ƒç­–ç•¥ ===")
    logging.info("æ•°æ®å·²æ‰‹åŠ¨å¹³è¡¡ï¼Œé‡‡ç”¨æ¸©å’Œæƒé‡ + Focal Lossç­–ç•¥")
    logging.info("é¢„æœŸæ•ˆæœï¼šæ‰€æœ‰ç±»åˆ«éƒ½èƒ½æœ‰æ•ˆå­¦ä¹ ")
    
    return None  # ç®€åŒ–è¿”å›ï¼Œä¸éœ€è¦å¤æ‚çš„é‡å¤ç­–ç•¥

def apply_progressive_learning(epoch, total_epochs, loss_fn):
    """ç®€åŒ–çš„å­¦ä¹ ç­–ç•¥ - æ•°æ®å¹³è¡¡å"""
    progress = epoch / total_epochs
    
    if progress < 0.5:
        # å‰åŠæ®µï¼šç¨³å®šå­¦ä¹ 
        focus_mode = "stable_learning"
        if epoch % 5 == 0:  # å‡å°‘æ—¥å¿—é¢‘ç‡
            logging.info(f"Epoch {epoch}: ç¨³å®šå­¦ä¹ é˜¶æ®µ")
    else:
        # ååŠæ®µï¼šç²¾ç»†è°ƒä¼˜
        focus_mode = "fine_tuning"
        if epoch % 5 == 0:  # å‡å°‘æ—¥å¿—é¢‘ç‡
            logging.info(f"Epoch {epoch}: ç²¾ç»†è°ƒä¼˜é˜¶æ®µ")
    
    return focus_mode

def analyze_dataset_distribution(dataset_path):
    """æ·±åº¦åˆ†ææ•°æ®é›†åˆ†å¸ƒï¼Œè¯†åˆ«æ½œåœ¨é—®é¢˜"""
    import os
    from collections import defaultdict
    from PIL import Image
    import numpy as np
    
    class_stats = defaultdict(lambda: {
        'count': 0,
        'avg_size': [],
        'avg_brightness': [],
        'image_modes': [],
        'file_sizes': []
    })
    
    print("\n=== æ•°æ®é›†æ·±åº¦åˆ†æ ===")
    
    for class_name in os.listdir(dataset_path):
        class_path = os.path.join(dataset_path, class_name)
        if not os.path.isdir(class_path):
            continue
            
        stats = class_stats[class_name]
        valid_images = 0
        
        for img_file in os.listdir(class_path):
            if not img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                continue
                
            img_path = os.path.join(class_path, img_file)
            try:
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(img_path)
                stats['file_sizes'].append(file_size)
                
                # åˆ†æå›¾åƒå±æ€§
                with Image.open(img_path) as img:
                    # å›¾åƒå°ºå¯¸
                    stats['avg_size'].append(img.size)
                    
                    # å›¾åƒæ¨¡å¼
                    stats['image_modes'].append(img.mode)
                    
                    # è½¬æ¢ä¸ºRGBåˆ†æäº®åº¦
                    if img.mode != 'RGB':
                        img = img.convert('RGB')
                    
                    # è®¡ç®—å¹³å‡äº®åº¦
                    img_array = np.array(img)
                    brightness = np.mean(img_array)
                    stats['avg_brightness'].append(brightness)
                    
                    valid_images += 1
                    
            except Exception as e:
                print(f"Error processing {img_path}: {e}")
        
        stats['count'] = valid_images
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    total_images = sum(stats['count'] for stats in class_stats.values())
    
    print(f"æ€»å›¾åƒæ•°: {total_images}")
    print(f"ç±»åˆ«æ•°: {len(class_stats)}")
    print("\nå„ç±»åˆ«è¯¦ç»†ç»Ÿè®¡:")
    
    problematic_classes = []
    
    for class_name, stats in class_stats.items():
        if stats['count'] == 0:
            continue
            
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_brightness = np.mean(stats['avg_brightness'])
        brightness_std = np.std(stats['avg_brightness'])
        
        avg_file_size = np.mean(stats['file_sizes'])
        file_size_std = np.std(stats['file_sizes'])
        
        unique_modes = set(stats['image_modes'])
        avg_width = np.mean([size[0] for size in stats['avg_size']])
        avg_height = np.mean([size[1] for size in stats['avg_size']])
        
        percentage = stats['count'] / total_images * 100
        
        # è¯†åˆ«æ½œåœ¨é—®é¢˜
        issues = []
        if brightness_std > 50:  # äº®åº¦å˜åŒ–å¤§
            issues.append("äº®åº¦å·®å¼‚å¤§")
        if len(unique_modes) > 1:  # å¤šç§å›¾åƒæ¨¡å¼
            issues.append("å›¾åƒæ¨¡å¼ä¸ç»Ÿä¸€")
        if file_size_std > avg_file_size:  # æ–‡ä»¶å¤§å°å·®å¼‚å¤§
            issues.append("æ–‡ä»¶å¤§å°å·®å¼‚å¤§")
        if avg_brightness < 50:  # å›¾åƒåæš—
            issues.append("å›¾åƒåæš—")
        if avg_brightness > 200:  # å›¾åƒåäº®
            issues.append("å›¾åƒåäº®")
            
        if issues:
            problematic_classes.append((class_name, issues))
        
        print(f"\n{class_name}:")
        print(f"  æ•°é‡: {stats['count']} ({percentage:.1f}%)")
        print(f"  å¹³å‡äº®åº¦: {avg_brightness:.1f} Â± {brightness_std:.1f}")
        print(f"  å¹³å‡å°ºå¯¸: {avg_width:.0f}x{avg_height:.0f}")
        print(f"  å›¾åƒæ¨¡å¼: {unique_modes}")
        print(f"  å¹³å‡æ–‡ä»¶å¤§å°: {avg_file_size/1024:.1f}KB Â± {file_size_std/1024:.1f}KB")
        if issues:
            print(f"  âš ï¸  æ½œåœ¨é—®é¢˜: {', '.join(issues)}")
    
    if problematic_classes:
        print(f"\nğŸš¨ å‘ç° {len(problematic_classes)} ä¸ªé—®é¢˜ç±»åˆ«:")
        for class_name, issues in problematic_classes:
            print(f"  {class_name}: {', '.join(issues)}")
    
    return class_stats, problematic_classes

def create_improved_loss_function(class_weights, num_classes):
    """åˆ›å»ºæ”¹è¿›çš„æŸå¤±å‡½æ•°"""
    
    class AdaptiveFocalLoss(nn.Cell):
        """è‡ªé€‚åº”Focal Loss - æ ¹æ®è®­ç»ƒè¿›åº¦è°ƒæ•´å‚æ•°"""
        def __init__(self, num_classes, class_weights, alpha=1.0, gamma=2.0):
            super(AdaptiveFocalLoss, self).__init__()
            self.num_classes = num_classes
            self.alpha = alpha
            self.gamma = gamma
            self.class_weights = ms.Tensor(class_weights, dtype=ms.float32)
            
            # MindSporeæ“ä½œ
            self.softmax = ms.ops.Softmax(axis=1)
            self.log_softmax = ms.ops.LogSoftmax(axis=1)
            self.onehot = ms.ops.OneHot()
            self.gather = ms.ops.GatherD()
            self.pow = ms.ops.Pow()
            
        def construct(self, logits, targets):
            # è®¡ç®—logæ¦‚ç‡
            log_probs = self.log_softmax(logits)
            probs = self.softmax(logits)
            
            # åˆ›å»ºone-hotç¼–ç 
            targets_one_hot = self.onehot(targets, self.num_classes, 
                                         ms.Tensor(1.0, ms.float32), 
                                         ms.Tensor(0.0, ms.float32))
            
            # è·å–ç›®æ ‡ç±»åˆ«çš„æ¦‚ç‡å’Œlogæ¦‚ç‡
            p_t = (probs * targets_one_hot).sum(axis=1)
            log_p_t = (log_probs * targets_one_hot).sum(axis=1)
            
            # è®¡ç®—focalæƒé‡ - æ›´æ¸©å’Œçš„å‚æ•°
            focal_weight = self.pow((1.0 - p_t), self.gamma)
            
            # åº”ç”¨ç±»åˆ«æƒé‡
            class_weight_t = self.gather(self.class_weights, 0, targets.view(-1))
            
            # è®¡ç®—focal loss
            focal_loss = -self.alpha * class_weight_t * focal_weight * log_p_t
            
            # æ·»åŠ æ ‡å‡†äº¤å‰ç†µä½œä¸ºç¨³å®šé¡¹
            ce_loss = -log_p_t
            
            # ç»„åˆæŸå¤±ï¼šä¸»è¦focal loss + å°‘é‡æ ‡å‡†CE
            total_loss = 0.8 * focal_loss + 0.2 * ce_loss * class_weight_t
            
            return total_loss.mean()
    
    return AdaptiveFocalLoss(num_classes, class_weights, alpha=0.75, gamma=1.5)

def create_enhanced_model():
    """åˆ›å»ºå¢å¼ºçš„æ¨¡å‹æ¶æ„"""
    
    class MedicalResNet(nn.Cell):
        """åŒ»å­¦å›¾åƒä¸“ç”¨ResNet"""
        def __init__(self, num_classes=8):
            super(MedicalResNet, self).__init__()
            
            # æ›´é€‚åˆåŒ»å­¦å›¾åƒçš„åˆå§‹å±‚
            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, pad_mode='pad')
            self.bn1 = nn.BatchNorm2d(64)
            self.relu = nn.ReLU()
            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode='same')
            
            # æ„å»ºResNetå±‚
            self.in_channels = 64
            self.layer1 = self._make_layer(64, 2, stride=1)
            self.layer2 = self._make_layer(128, 2, stride=2)
            self.layer3 = self._make_layer(256, 2, stride=2)
            self.layer4 = self._make_layer(512, 2, stride=2)
            
            # å…¨å±€å¹³å‡æ± åŒ–
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            
            # æ›´å¼ºçš„åˆ†ç±»å™¨
            self.classifier = nn.SequentialCell([
                nn.Dense(512, 512),
                nn.BatchNorm1d(512),
                nn.ReLU(),
                nn.Dropout(p=0.5),
                
                nn.Dense(512, 256),
                nn.BatchNorm1d(256),
                nn.ReLU(),
                nn.Dropout(p=0.3),
                
                nn.Dense(256, num_classes)
            ])
            
        def _make_layer(self, out_channels, blocks, stride=1):
            layers = []
            
            # ç¬¬ä¸€ä¸ªblockå¯èƒ½éœ€è¦ä¸‹é‡‡æ ·
            layers.append(self._make_block(self.in_channels, out_channels, stride))
            self.in_channels = out_channels
            
            # å…¶ä½™blocks
            for _ in range(1, blocks):
                layers.append(self._make_block(out_channels, out_channels, 1))
                
            return nn.SequentialCell(layers)
        
        def _make_block(self, in_channels, out_channels, stride):
            """åˆ›å»ºResNetåŸºæœ¬å—"""
            downsample = None
            if stride != 1 or in_channels != out_channels:
                downsample = nn.SequentialCell([
                    nn.Conv2d(in_channels, out_channels, 1, stride=stride),
                    nn.BatchNorm2d(out_channels)
                ])
            
            return BasicResBlock(in_channels, out_channels, stride, downsample)
        
        def construct(self, x):
            x = self.conv1(x)
            x = self.bn1(x)
            x = self.relu(x)
            x = self.maxpool(x)
            
            x = self.layer1(x)
            x = self.layer2(x)
            x = self.layer3(x)
            x = self.layer4(x)
            
            x = self.avgpool(x)
            x = P.Flatten()(x)
            x = self.classifier(x)
            
            return x
    
    class BasicResBlock(nn.Cell):
        """åŸºæœ¬ResNetå—"""
        def __init__(self, in_channels, out_channels, stride=1, downsample=None):
            super(BasicResBlock, self).__init__()
            self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, pad_mode='pad')
            self.bn1 = nn.BatchNorm2d(out_channels)
            self.relu = nn.ReLU()
            self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, pad_mode='pad')
            self.bn2 = nn.BatchNorm2d(out_channels)
            self.downsample = downsample
            
        def construct(self, x):
            identity = x
            
            out = self.conv1(x)
            out = self.bn1(out)
            out = self.relu(out)
            
            out = self.conv2(out)
            out = self.bn2(out)
            
            if self.downsample is not None:
                identity = self.downsample(x)
                
            out = out + identity
            out = self.relu(out)
            
            return out
    
    return MedicalResNet()

def improved_training_strategy():
    """æ”¹è¿›çš„è®­ç»ƒç­–ç•¥"""
    
    class WarmupCosineScheduler:
        """é¢„çƒ­+ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        def __init__(self, max_lr, min_lr, warmup_epochs, total_epochs, steps_per_epoch):
            self.max_lr = max_lr
            self.min_lr = min_lr
            self.warmup_epochs = warmup_epochs
            self.total_epochs = total_epochs
            self.steps_per_epoch = steps_per_epoch
            self.warmup_steps = warmup_epochs * steps_per_epoch
            self.total_steps = total_epochs * steps_per_epoch
            
        def get_lr(self, step):
            if step < self.warmup_steps:
                # é¢„çƒ­é˜¶æ®µï¼šçº¿æ€§å¢åŠ 
                return self.min_lr + (self.max_lr - self.min_lr) * step / self.warmup_steps
            else:
                # ä½™å¼¦é€€ç«é˜¶æ®µ
                progress = (step - self.warmup_steps) / (self.total_steps - self.warmup_steps)
                return self.min_lr + (self.max_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))
    
    return WarmupCosineScheduler(
        max_lr=0.001,  # ç¨å¾®æé«˜æœ€å¤§å­¦ä¹ ç‡
        min_lr=1e-6,
        warmup_epochs=3,  # 3ä¸ªepoché¢„çƒ­
        total_epochs=Config.EPOCHS,
        steps_per_epoch=100
    )

def train():
    """è®­ç»ƒæ¨¡å‹ - æ”¹è¿›ç‰ˆæœ¬"""
    # è®¾ç½®è¿è¡Œç¯å¢ƒ
    ms.set_device(Config.DEVICE_TARGET)
    context.set_context(mode=context.PYNATIVE_MODE)
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs(Config.LOG_DIR, exist_ok=True)
    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    log_file = os.path.join(Config.LOG_DIR, f'train_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    
    # æ·±åº¦åˆ†ææ•°æ®é›†
    logging.info("=== å¼€å§‹æ•°æ®é›†æ·±åº¦åˆ†æ ===")
    class_stats, problematic_classes = analyze_dataset_distribution(Config.TRAIN_DATA_PATH)
    
    if problematic_classes:
        logging.warning(f"å‘ç° {len(problematic_classes)} ä¸ªé—®é¢˜ç±»åˆ«ï¼Œè¿™å¯èƒ½å½±å“è®­ç»ƒæ•ˆæœ")
        for class_name, issues in problematic_classes:
            logging.warning(f"  {class_name}: {', '.join(issues)}")
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    class_weights = calculate_class_weights(Config.TRAIN_DATA_PATH)
    logging.info("Class weights: %s", class_weights)
    
    # è·å–å¹³è¡¡è®­ç»ƒç­–ç•¥
    repeat_factors = get_balanced_training_strategy(None, class_weights)
    
    # è®°å½•è®­ç»ƒé…ç½®
    logging.info("Training Configuration:")
    logging.info(json.dumps(Config.to_dict(), indent=2))
    
    # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
    config_path = os.path.join(Config.LOG_DIR, f'config_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
    Config.save_config(config_path)
    
    # è·å–æ•°æ®é›† - æ ¹æ®æ¨¡å¼é€‰æ‹©
    if getattr(Config, 'FAST_TRAINING', False):
        logging.info("Using fast dataset mode (simplified augmentation)")
        train_dataset = get_fast_dataset(Config.TRAIN_DATA_PATH, is_training=True)
        valid_dataset = get_fast_dataset(Config.VALID_DATA_PATH, is_training=False)
    else:
        logging.info("Using full dataset mode (enhanced augmentation)")
        train_dataset = get_dataset(Config.TRAIN_DATA_PATH, is_training=True)
        valid_dataset = get_dataset(Config.VALID_DATA_PATH, is_training=False)
    
    # å¿«é€Ÿè®­ç»ƒæ¨¡å¼ï¼šé™åˆ¶æ¯ä¸ªepochçš„æ­¥æ•°
    full_steps = train_dataset.get_dataset_size()
    if full_steps > Config.TARGET_STEPS_PER_EPOCH:
        logging.info(f"Fast training mode: reducing steps from {full_steps} to {Config.TARGET_STEPS_PER_EPOCH} per epoch")
        train_dataset = train_dataset.take(Config.TARGET_STEPS_PER_EPOCH)
    else:
        logging.info(f"Using full dataset: {full_steps} steps per epoch")
    
    # è·å–æ”¹è¿›çš„æ¨¡å‹
    try:
        network = create_enhanced_model()
        logging.info("ä½¿ç”¨åŒ»å­¦å›¾åƒä¸“ç”¨ResNetæ¨¡å‹")
    except Exception as e:
        logging.warning(f"åˆ›å»ºå¢å¼ºæ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹: {e}")
        network = get_model()
    
    # è·å–æ”¹è¿›çš„å­¦ä¹ ç‡è°ƒåº¦å™¨
    steps_per_epoch = train_dataset.get_dataset_size()
    logging.info(f"Steps per epoch: {steps_per_epoch}")
    
    lr_scheduler = improved_training_strategy()
    
    # åŠ¨æ€å­¦ä¹ ç‡
    def get_dynamic_lr(step):
        return lr_scheduler.get_lr(step)
    
    # åˆ›å»ºå­¦ä¹ ç‡åºåˆ—
    lr_values = []
    for step in range(Config.EPOCHS * steps_per_epoch):
        lr_values.append(get_dynamic_lr(step))
    
    # å®šä¹‰ä¼˜åŒ–å™¨ - æ”¹è¿›å‚æ•°
    # ä½¿ç”¨æ›´ç¨³å®šçš„SGDä¼˜åŒ–å™¨
    optimizer = nn.SGD(
        network.trainable_params(),
        learning_rate=ms.Tensor(lr_values, dtype=ms.float32),
        momentum=Config.MOMENTUM,
        weight_decay=Config.WEIGHT_DECAY
    )
    
    # ä½¿ç”¨æ”¹è¿›çš„æŸå¤±å‡½æ•°
    loss_fn = create_improved_loss_function(class_weights, Config.NUM_CLASSES)
    logging.info("ä½¿ç”¨è‡ªé€‚åº”Focal LossæŸå¤±å‡½æ•°")
    
    # å®šä¹‰è®­ç»ƒæ­¥éª¤
    def forward_fn(data, label):
        logits = network(data)
        loss = loss_fn(logits, label)
        return loss, logits
    
    # å®šä¹‰æ¢¯åº¦å‡½æ•°
    grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)
    
    # å®šä¹‰è®­ç»ƒæ­¥éª¤
    def train_step(data, label):
        (loss, _), grads = grad_fn(data, label)
        optimizer(grads)
        return loss
    
    # è®­ç»ƒå¾ªç¯
    best_acc = 0.0
    no_improve_epochs = 0
    start_time = time.time()
    
    for epoch in range(Config.EPOCHS):
        # åº”ç”¨ç®€åŒ–çš„å­¦ä¹ ç­–ç•¥
        focus_mode = apply_progressive_learning(epoch, Config.EPOCHS, loss_fn)
        
        # è®­ç»ƒé˜¶æ®µ
        network.set_train()
        total_loss = 0
        step_time = time.time()
        
        for step, (data, label) in enumerate(train_dataset.create_tuple_iterator()):
            loss = train_step(data, label)
            total_loss += loss.asnumpy()
            
            # å¿«é€Ÿè®­ç»ƒæ¨¡å¼ä¸‹å‡å°‘æ—¥å¿—é¢‘ç‡
            log_interval = 20 if getattr(Config, 'FAST_TRAINING', False) else 10
            if (step + 1) % log_interval == 0:
                step_time = time.time() - step_time
                logging.info(f"epoch: {epoch + 1} step: {step + 1}, loss is {loss.asnumpy()}")
                logging.info(f"step time: {step_time * 1000:.2f} ms")
                step_time = time.time()
        
        # éªŒè¯é˜¶æ®µ
        network.set_train(False)
        correct = 0
        total = 0
        class_correct = [0] * Config.NUM_CLASSES
        class_total = [0] * Config.NUM_CLASSES
        
        for data, label in valid_dataset.create_tuple_iterator():
            logits = network(data)
            pred = ms.ops.Argmax(axis=1)(logits)
            correct += (pred == label).sum().asnumpy()
            total += label.shape[0]
            
            # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
            pred_numpy = pred.asnumpy()
            label_numpy = label.asnumpy()
            
            for i in range(Config.NUM_CLASSES):
                class_mask = (label_numpy == i)
                class_total[i] += np.sum(class_mask)
                
                if np.sum(class_mask) > 0:
                    class_predictions = pred_numpy[class_mask]
                    class_correct[i] += np.sum(class_predictions == i)
        
        accuracy = correct / total if total > 0 else 0
        logging.info(f"Validation accuracy: {accuracy:.2%}")
        
        # æ‰“å°æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        non_zero_classes = 0
        for i in range(Config.NUM_CLASSES):
            if class_total[i] > 0:
                class_acc = class_correct[i] / class_total[i]
                class_acc = max(0.0, min(1.0, class_acc))
                status = "[OK]" if class_acc > 0 else "[NO]"
                logging.info(f"Class {Config.CLASS_NAMES[i]} accuracy: {class_acc:.2%} ({class_correct[i]}/{class_total[i]}) {status}")
                
                if class_acc > 0:
                    non_zero_classes += 1
            else:
                logging.info(f"Class {Config.CLASS_NAMES[i]} accuracy: N/A (no samples)")
        
        # æ˜¾ç¤ºæœ‰å­¦ä¹ æ•ˆæœçš„ç±»åˆ«æ•°é‡
        logging.info(f"Learning classes: {non_zero_classes}/{Config.NUM_CLASSES}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if accuracy > best_acc:
            best_acc = accuracy
            no_improve_epochs = 0
            best_ckpt_file = os.path.join(Config.CHECKPOINT_DIR, f"{Config.MODEL_PREFIX}_best.ckpt")
            ms.save_checkpoint(network, best_ckpt_file)
            logging.info(f"æ•´ä½“å‡†ç¡®ç‡æå‡åˆ°: {accuracy:.2%} - å·²ä¿å­˜æœ€ä½³æ¨¡å‹")
        else:
            no_improve_epochs += 1
        
        # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹
        if (epoch + 1) % 5 == 0:
            epoch_ckpt_file = os.path.join(Config.CHECKPOINT_DIR, f"{Config.MODEL_PREFIX}_epoch_{epoch+1}.ckpt")
            ms.save_checkpoint(network, epoch_ckpt_file)
            logging.info(f"Saved epoch {epoch+1} model")
        
        # æ—©åœæ£€æŸ¥
        if no_improve_epochs >= Config.EARLY_STOPPING_PATIENCE:
            logging.info(f"Early stopping triggered after {epoch + 1} epochs")
            break
    
    # è®°å½•è®­ç»ƒç»“æœ
    training_time = time.time() - start_time
    logging.info(f"Training completed in {training_time/3600:.2f} hours")
    logging.info(f"Best validation accuracy: {best_acc:.2%}")
    
    # ä¿å­˜è®­ç»ƒé…ç½®å’Œç»“æœ
    results = {
        "best_accuracy": float(best_acc),
        "training_time": float(training_time),
        "epochs_trained": epoch + 1,
        "final_epoch": epoch + 1,
        "early_stopped": no_improve_epochs >= Config.EARLY_STOPPING_PATIENCE,
        "class_weights": class_weights.tolist()
    }
    
    with open(os.path.join(Config.LOG_DIR, 'training_results.json'), 'w') as f:
        json.dump(results, f, indent=2)

if __name__ == '__main__':
    train()